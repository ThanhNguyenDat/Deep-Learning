{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47459913",
   "metadata": {},
   "source": [
    "Deep Learning system are often trained on **very large datasets.**\n",
    "\n",
    "Tensorflow make easy:\n",
    "* Data API\n",
    "* TF Transform\n",
    "* TFRecord\n",
    "* TF Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869d289",
   "metadata": {},
   "source": [
    "# TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3cf83d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● apply()              Applies a transformation function to this dataset.\n",
      "● as_numpy_iterator()  Returns an iterator which converts all elements of the dataset to numpy.\n",
      "● batch()              Combines consecutive elements of this dataset into batches.\n",
      "● cache()              Caches the elements in this dataset.\n",
      "● cardinality()        Returns the cardinality of the dataset, if known.\n",
      "● concatenate()        Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
      "● element_spec()       The type specification of an element of this dataset.\n",
      "● enumerate()          Enumerates the elements of this dataset.\n",
      "● filter()             Filters this dataset according to `predicate`.\n",
      "● flat_map()           Maps `map_func` across this dataset and flattens the result.\n",
      "● from_generator()     Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
      "● from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors.\n",
      "● from_tensors()       Creates a `Dataset` with a single element, comprising the given tensors.\n",
      "● interleave()         Maps `map_func` across this dataset, and interleaves the results.\n",
      "● list_files()         A dataset of all files matching one or more glob patterns.\n",
      "● map()                Maps `map_func` across the elements of this dataset.\n",
      "● options()            Returns the options for this dataset and its inputs.\n",
      "● padded_batch()       Combines consecutive elements of this dataset into padded batches.\n",
      "● prefetch()           Creates a `Dataset` that prefetches elements from this dataset.\n",
      "● range()              Creates a `Dataset` of a step-separated range of values.\n",
      "● reduce()             Reduces the input dataset to a single element.\n",
      "● repeat()             Repeats this dataset so each original value is seen `count` times.\n",
      "● shard()              Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      "● shuffle()            Randomly shuffles the elements of this dataset.\n",
      "● skip()               Creates a `Dataset` that skips `count` elements from this dataset.\n",
      "● take()               Creates a `Dataset` with at most `count` elements from this dataset.\n",
      "● unbatch()            Splits elements of a dataset into multiple elements.\n",
      "● window()             Combines (nests of) input elements into a dataset of (nests of) windows.\n",
      "● with_options()       Returns a new `tf.data.Dataset` with the given options set.\n",
      "● zip()                Creates a `Dataset` by zipping together the given datasets.\n"
     ]
    }
   ],
   "source": [
    "for m in dir(tf.data.Dataset):\n",
    "    if not (m.startswith(\"_\") or m.endswith(\"_\")):\n",
    "        func = getattr(tf.data.Dataset, m)\n",
    "        if hasattr(func, \"__doc__\"):\n",
    "            print(\"● {:21s}{}\".format(m + \"()\", func.__doc__.split(\"\\n\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db8a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd801ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = [11, 22, 33, 44, 55, 66, 77, 88]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(sample_data) # convert data into tf data\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c7f6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "22\n",
      "33\n",
      "44\n",
      "55\n",
      "66\n",
      "77\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    # print(item)\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3abcbb",
   "metadata": {},
   "source": [
    "## Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aecb4830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(44, shape=(), dtype=int32)\n",
      "tf.Tensor(55, shape=(), dtype=int32)\n",
      "tf.Tensor(66, shape=(), dtype=int32)\n",
      "tf.Tensor(77, shape=(), dtype=int32)\n",
      "tf.Tensor(88, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Filter data\n",
    "dataset = dataset.filter(lambda sample:sample>33)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554969d5",
   "metadata": {},
   "source": [
    "## Data transformations\n",
    "NOTE: these functions are NOT in-place, hence require re-assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20fd103b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (None,), types: tf.int32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = dataset.repeat(3) # duplicate data\n",
    "# dataset = dataset.batch(7) # group data. drop_remainder=True: drop the last batch\n",
    "dataset = dataset.repeat(3).batch(7) # do above 2 transformations at once\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329fa5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([44 55 66 77 88 44 55], shape=(7,), dtype=int32)\n",
      "tf.Tensor([66 77 88 44 55 66 77], shape=(7,), dtype=int32)\n",
      "tf.Tensor([88], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8090e0e",
   "metadata": {},
   "source": [
    "## Custom transformation on EACH SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad3e2da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_UnbatchDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom transformation on Each Sample\n",
    "dataset = dataset.unbatch() # ungroup data\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b755a",
   "metadata": {},
   "source": [
    "### Sysntax 1 (only for simple transform)\n",
    "Note: num_paralled_calls = #threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90545d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.map(lambda sample: sample*10 if sample<60 else sample, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fc858",
   "metadata": {},
   "source": [
    "### Sysntax 2 (CLEAREST & most flexible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c0505d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func_sample(x):\n",
    "    if x < 60:\n",
    "        x = x * 10\n",
    "    return x\n",
    "dataset = dataset.map(lambda sample: my_func_sample(sample), num_parallel_calls=4)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20a4577",
   "metadata": {},
   "source": [
    "### Sysntax 3 (short form of sysntax 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a5d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(my_func_sample, num_parallel_calls=4) \n",
    "#drawback: only pass 1 parameter (sample), do not pass another parameter to the my_func_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "407876dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(440, shape=(), dtype=int32)\n",
      "tf.Tensor(550, shape=(), dtype=int32)\n",
      "tf.Tensor(66, shape=(), dtype=int32)\n",
      "tf.Tensor(77, shape=(), dtype=int32)\n",
      "tf.Tensor(88, shape=(), dtype=int32)\n",
      "tf.Tensor(440, shape=(), dtype=int32)\n",
      "tf.Tensor(550, shape=(), dtype=int32)\n",
      "tf.Tensor(66, shape=(), dtype=int32)\n",
      "tf.Tensor(77, shape=(), dtype=int32)\n",
      "tf.Tensor(88, shape=(), dtype=int32)\n",
      "tf.Tensor(440, shape=(), dtype=int32)\n",
      "tf.Tensor(550, shape=(), dtype=int32)\n",
      "tf.Tensor(66, shape=(), dtype=int32)\n",
      "tf.Tensor(77, shape=(), dtype=int32)\n",
      "tf.Tensor(88, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ac6ea",
   "metadata": {},
   "source": [
    "## Custom transformation on WHOLE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ce429d",
   "metadata": {},
   "source": [
    "### Sysntax 1 (simple transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adecb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 100\n",
    "# dataset = dataset.apply(lambda datset: datset.filter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907dc81b",
   "metadata": {},
   "source": [
    "### Sysntax 2 (CLEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54e68bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(ds):\n",
    "    ds = ds.filter(lambda sample: sample < max_val)\n",
    "    new_ds = ds.map(lambda sample: sample / 10)\n",
    "    return new_ds\n",
    "\n",
    "dataset = dataset.apply(lambda datset: my_func(datset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0615b02",
   "metadata": {},
   "source": [
    "### Sysntax 3 (short form of syntax 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a553d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datset.apply(my_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4742ec06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.6, shape=(), dtype=float64)\n",
      "tf.Tensor(7.7, shape=(), dtype=float64)\n",
      "tf.Tensor(8.8, shape=(), dtype=float64)\n",
      "tf.Tensor(6.6, shape=(), dtype=float64)\n",
      "tf.Tensor(7.7, shape=(), dtype=float64)\n",
      "tf.Tensor(8.8, shape=(), dtype=float64)\n",
      "tf.Tensor(6.6, shape=(), dtype=float64)\n",
      "tf.Tensor(7.7, shape=(), dtype=float64)\n",
      "tf.Tensor(8.8, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10a32c",
   "metadata": {},
   "source": [
    "## Randomly shuffle the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f73e4",
   "metadata": {},
   "source": [
    "NOTE: shuffle dataset helps to make sure your training data are iid \n",
    "\n",
    "      (REQUIRED in training using gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfe40c",
   "metadata": {},
   "source": [
    "### 1. With small dataset\n",
    "INFO: shuffle() method works by getting N items of the dataset (N='buffer_size') into RAM each time. Then it randomly draws samples from this buffer and replaces the drawn ones with new samples from the dataset.\n",
    "\n",
    "NOTE: MUST set 'buffer_size' so that the buffet doesn't exceed RAM capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2b8981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.6, shape=(), dtype=float64)\n",
      "tf.Tensor(6.6, shape=(), dtype=float64)\n",
      "tf.Tensor(7.7, shape=(), dtype=float64)\n",
      "tf.Tensor(8.8, shape=(), dtype=float64)\n",
      "tf.Tensor(7.7, shape=(), dtype=float64)\n",
      "tf.Tensor(8.8, shape=(), dtype=float64)\n",
      "tf.Tensor(8.8, shape=(), dtype=float64)\n",
      "tf.Tensor(7.7, shape=(), dtype=float64)\n",
      "tf.Tensor(6.6, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(buffer_size=3, seed=42)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88760fb7",
   "metadata": {},
   "source": [
    "### 2. With LARGE dataset\n",
    "NOTE: shuffle() alone CAN'T shuffle well large dataset since the buffer size is relatively small compared to the dataset's size.\n",
    "#### Solution:\n",
    "* 1. Split dataset into muptiple files.\n",
    "* 2. Read these files at once to draw random samples from ALL parts of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c05ddeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e7c74",
   "metadata": {},
   "source": [
    "Demo on California housing dataset\n",
    "INFO: 20640 samples, 8 features, label values: 0.15 - 5.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb5f7569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cali housing dataset size: (20640, 8)\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "print(\"Cali housing dataset size:\",housing.data.shape)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85d5311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done writing files. Training file paths: ['datasets\\\\housing\\\\my_train_00.csv', 'datasets\\\\housing\\\\my_train_01.csv', 'datasets\\\\housing\\\\my_train_02.csv', 'datasets\\\\housing\\\\my_train_03.csv', 'datasets\\\\housing\\\\my_train_04.csv', 'datasets\\\\housing\\\\my_train_05.csv', 'datasets\\\\housing\\\\my_train_06.csv', 'datasets\\\\housing\\\\my_train_07.csv', 'datasets\\\\housing\\\\my_train_08.csv', 'datasets\\\\housing\\\\my_train_09.csv', 'datasets\\\\housing\\\\my_train_10.csv', 'datasets\\\\housing\\\\my_train_11.csv', 'datasets\\\\housing\\\\my_train_12.csv', 'datasets\\\\housing\\\\my_train_13.csv', 'datasets\\\\housing\\\\my_train_14.csv', 'datasets\\\\housing\\\\my_train_15.csv', 'datasets\\\\housing\\\\my_train_16.csv', 'datasets\\\\housing\\\\my_train_17.csv', 'datasets\\\\housing\\\\my_train_18.csv', 'datasets\\\\housing\\\\my_train_19.csv']\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Save data to multiple files \n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)\n",
    "print('\\nDone writing files. Training file paths:',train_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "006c5c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some first samples:\n",
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
      "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
      "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
      "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
      "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
      "\n",
      "   Longitude  MedianHouseValue  \n",
      "0    -122.43             1.442  \n",
      "1    -117.39             1.687  \n",
      "2    -122.98             1.621  \n",
      "3    -117.70             2.621  \n",
      "4    -116.93             0.956  \n",
      "\n",
      "File paths (in RANDOM ORDER):\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_08.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Read created files at once \n",
    "# Read some first samples of a file (just to see)\n",
    "import pandas as pd\n",
    "print('\\nSome first samples:')\n",
    "print(pd.read_csv(train_filepaths[0]).head())\n",
    "\n",
    "# Create a dataset containing file paths in RANDOM ORDER, using list_files() \n",
    "print('\\nFile paths (in RANDOM ORDER):')\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, shuffle=True, seed=42)\n",
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e063ef2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some first samples of the SHUFFLED dataset:\n",
      "b'4.5909,16.0,5.475877192982456,1.0964912280701755,1357.0,2.9758771929824563,33.63,-117.71,2.418'\n",
      "b'2.4792,24.0,3.4547038327526134,1.1341463414634145,2251.0,3.921602787456446,34.18,-118.38,2.0'\n",
      "b'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67'\n",
      "b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205'\n",
      "b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\n",
      "b'3.6548,29.0,4.6434540389972145,0.9916434540389972,2919.0,4.0654596100278555,34.3,-118.42,1.803'\n",
      "b'3.9543,35.0,5.134122287968442,0.9506903353057199,1305.0,2.57396449704142,33.94,-118.0,2.144'\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset containing data from MULTIPLE FILES, using interleave()\n",
    "# INFO: interleave() creates a dataset from N (N=cycle_length) RANDOM files (with names in 'filepath_dataset'). \n",
    "#       When you get data from this dataset, you will get first rows of these N random files (one row/file each time).\n",
    "#       When you get all rows from these N files, the other N files (from 'filepath_dataset') will be generated.\n",
    "# NOTE: files (with names in 'filepath_dataset') SHOULD have the identical length,\n",
    "#       otherwise the ends of the longer files won't be gotten.      \n",
    "N_files_1_read = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # skip(1): the header row\n",
    "    cycle_length=N_files_1_read)\n",
    "print('\\nSome first samples of the SHUFFLED dataset:')\n",
    "for item in dataset.take(7):\n",
    "    #print(item)\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0528af",
   "metadata": {},
   "source": [
    "## Converts CSV records to tensors using decode_csv()\n",
    "INFO: \n",
    "+ decode_csv() converts CSV records to tf tensors.\n",
    "+ A CSV record is a string with commas and NO space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd010a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_record = '11,22,33,44,55' # with NO missing values\n",
    "sample_record = ',22,,,55' # with MISSING values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47ba556c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.101>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=22.0>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'Hello'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=55.0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE on creating DEFAULT VALUES:\n",
    "#   + No. of default values MUST match exactly no. of fields in the records.\n",
    "#   + Empty default values (eg. tf.constant([])) mean the fields are REQUIRED (no missing allowed).\n",
    "default_values=[0.101, np.nan, tf.constant(np.nan, dtype=tf.float64), \"Hello\", tf.constant([])] \n",
    "#default_values=[0.101, np.nan, \"Hello\", tf.constant([])] # NOT enough values => ERROR\n",
    "processed_record = tf.io.decode_csv(sample_record, default_values)\n",
    "processed_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3e4b0",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d225cdc",
   "metadata": {},
   "source": [
    "### Processing 1 record\n",
    "eager mode  vs  graph mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f2fbb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample record:\n",
      "x = tf.Tensor(\n",
      "[   4.6477      38.           0.           0.9118644  745.\n",
      "    2.5254238   32.64      -117.07     ], shape=(8,), dtype=float32) \n",
      "y = tf.Tensor([1.504], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function # Convert below function to tf function (FASTER than regular python function)\n",
    "def preprocess(line, no_of_features=1):\n",
    "    default_val = [0.]*no_of_features + [tf.constant([], dtype=tf.float32)] # last field is the label (CAN'T be missing)\n",
    "    fields = tf.io.decode_csv(line, record_defaults=default_val)\n",
    "    x = tf.stack(fields[:-1]) # tf.stack(): merges elements into 1 array\n",
    "    y = tf.stack(fields[-1:])\n",
    "    # Do other preprocessing here...\n",
    "    return x, y\n",
    "no_of_features = X_train.shape[-1]\n",
    "sample_record = b'4.6477,38,,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504' # b: string of byte literals\n",
    "(x,y) = preprocess(sample_record, no_of_features)\n",
    "print('\\nSample record:\\nx =',x,'\\ny =',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c390c",
   "metadata": {},
   "source": [
    "## Preprocessing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a07249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processed training data:\n",
      "\n",
      " [ 2.0800e+00  3.6000e+01  4.5200e+00  1.0500e+00  1.4690e+03  4.9000e+00\n",
      "  3.3920e+01 -1.1819e+02]\n",
      "\n",
      " [   3.14   33.      5.04    1.02  814.      2.48   37.95 -122.04]\n"
     ]
    }
   ],
   "source": [
    "def csv_reader_dataset(filepaths, no_of_features, line_skip=1, no_files_1_read=5,\n",
    "                       num_threads=1, shuffle_buffer_size=10000, batch_size=32):\n",
    "    # Create a dataset of file paths:\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    # Create a dataset of shuffled samples from files (in the 'filepaths'):\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(line_skip),\n",
    "        cycle_length=no_files_1_read, num_parallel_calls=num_threads)\n",
    "    # Cache the data in RAM for speed:\n",
    "    dataset = dataset.cache() # NOTE: dataset size MUST <= RAM capacity\n",
    "    # Shuffle records 1 more time:\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    # Preprocess records:\n",
    "    dataset = dataset.map(lambda line: preprocess(line,no_of_features), num_parallel_calls=num_threads)\n",
    "    # Group records into batches:\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1) # NOTE: prefetch(): can increase running speed significantly, by preparing the data AHEAD of calling.\n",
    "dataset = csv_reader_dataset(train_filepaths, no_of_features, batch_size=2)\n",
    "print('\\n\\nProcessed training data:')\n",
    "for record in dataset.take(2):\n",
    "    #print('\\n',record)    \n",
    "    print('\\n',np.round(record[0][0],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed859f",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c838364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 173.1734 - val_loss: 79.1051\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 49.4811 - val_loss: 14.7093\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 11.9471 - val_loss: 9.4751\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 9.0073 - val_loss: 9.1896\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 8.3039 - val_loss: 8.6799\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 7.7297 - val_loss: 8.8952\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7.6161 - val_loss: 8.1150\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 6.6638 - val_loss: 10.5360\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 8.4859 - val_loss: 5.8107\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 6.6986 - val_loss: 6.4713\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "n_inputs = X_train.shape[-1]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_\n",
    "\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation='elu', kernel_initializer=\"he_uniform\", input_shape=X_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='mean_absolute_error', optimizer=opt)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train, y_train, \\\n",
    "         epochs=10, steps_per_epoch=len(X_train) // batch_size, \\\n",
    "         validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cf0eaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `model.fit` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1faf5f62",
   "metadata": {},
   "source": [
    "## Evaluate the housing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8574742b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1233 test_function  *\n        return step_function(self, iterator)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1217 run_step  **\n        outputs = model.test_step(data)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1183 test_step\n        y_pred = self(x, training=False)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:207 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer sequential_20 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 13) dtype=float32>, <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3252/2119601362.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model = keras.models.load_model(r'models/housing_best.h5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# NOTE: new_data is in fact a data-random-sampling object,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1387\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1389\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1390\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m-> 2941\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[0;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[1;32m-> 3358\u001b[1;33m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 3280\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1233 test_function  *\n        return step_function(self, iterator)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1217 run_step  **\n        outputs = model.test_step(data)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1183 test_step\n        y_pred = self(x, training=False)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:207 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer sequential_20 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 13) dtype=float32>, <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# model = keras.models.load_model(r'models/housing_best.h5')\n",
    "model.evaluate(test_set)\n",
    "new_data = test_set.take(1)\n",
    "# NOTE: new_data is in fact a data-random-sampling object, \n",
    "#       hence each time we \"touch\" it, it generate new samples.\n",
    "#       => For result comparison, we need to store its generated data to a list.\n",
    "new_x_fixed = []\n",
    "new_y_fixed = []\n",
    "for (x,y) in new_data:\n",
    "    new_x_fixed.append(x)\n",
    "    new_y_fixed.append(y)\n",
    "print('\\nPredictions:\\n',model.predict(new_x_fixed)[:5])\n",
    "print('\\nTrue labels:')\n",
    "for y in new_y_fixed:\n",
    "    print(y.numpy()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b979f3",
   "metadata": {},
   "source": [
    "## WRITE YOUR OWN TRAINING LOOP FUNCTION\n",
    "Original code: cell 40 in '13_loading_and_preprocessing_data.ipynb'\n",
    "NOTE: requires knowledge custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "689c30ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    C:\\Users\\84766\\AppData\\Local\\Temp/ipykernel_3252/2213743708.py:7 train  *\n        train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n\n    TypeError: tf__csv_reader_dataset() got an unexpected keyword argument 'repeat'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3252/2276162921.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# n_epochs=2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 726\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    C:\\Users\\84766\\AppData\\Local\\Temp/ipykernel_3252/2213743708.py:7 train  *\n        train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n\n    TypeError: tf__csv_reader_dataset() got an unexpected keyword argument 'repeat'\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train(model, n_epochs, train_filepaths, no_of_features, optimizer, loss_fn, \n",
    "          batch_size=32, no_files_1_read=5, num_threads=1, shuffle_buffer_size=10000):\n",
    "    train_set = csv_reader_dataset(train_filepaths, no_of_features, no_files_1_read=no_files_1_read,\n",
    "                                   num_threads=num_threads, shuffle_buffer_size=shuffle_buffer_size, batch_size=batch_size)\n",
    "    for epoch in tf.range(n_epochs):\n",
    "        iter = 0\n",
    "        for X_batch, y_batch in train_set:\n",
    "            iter += 1\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(X_batch)\n",
    "                main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                loss = tf.add_n([main_loss] + model.losses)\n",
    "                tf.print(\"\\rEpoch \", epoch+1, \"/\", n_epochs,', iter ',iter,': loss = ',loss,sep='')\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            n_epochs=2\n",
    "no_of_features = X_train.shape[-1] # = 8\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "train(model, n_epochs, train_filepaths, no_of_features, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bacb94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
